<?php

declare(strict_types=1);

namespace NeuralNet\Net;

/**
 * Сети прямого распространения (Feed forward neural networks, FF or FFNN) и
 * перцептроны (perceptrons, P) очень просты — они передают информацию от входа к выходу.
 * Считается, что у нейронных сетей есть слои, каждый из которых состоит из входных,
 * скрытых или выходных нейронов. Нейроны одного слоя между собой не связаны, при этом
 * каждый нейрон этого слоя связан с каждым нейроном соседнего слоя.
 * Простейшая мало-мальски рабочая сеть состоит из двух входных и одного выходного
 * нейрона и может моделировать логический вентиль — базовый элемент цифровой схемы,
 * выполняющий элементарную логическую операцию. FFNN обычно обучают методом обратного
 * распространения ошибки, подавая модели на вход пары входных и ожидаемых выходных данных.
 * Под ошибкой обычно понимаются различные степени отклонения выходных данных от
 * исходных (например, среднеквадратичное отклонение или сумма модулей разностей).
 * При условии, что сеть обладает достаточным количеством скрытых нейронов,
 * теоретически она всегда сможет установить связь между входными и выходными данными.
 * На практике использование сетей прямого распространения ограничено, и чаще они
 * используются совместно с другими сетями.
 * Сети радиально-базисных функций (radial basis function, RBF) — это FFNN с
 * радиально-базисной функцией в качестве функции активации.
 *
 * @author      Gordon Freeman <toxa82@gmail.com>
 * @copyright   Copyright (c) 2016 Gordon Freeman
 */
class FeedForward extends Base
{

    /**
     * @param \NeuralNet\Layer\Base[] $layers Массив слоёв сети
     */
    public function __construct(array $layers)
    {
        foreach ($layers as $layer) {
            if (!$layer instanceof \NeuralNet\Layer\Base) {
                throw new \InvalidArgumentException(\sprintf('Layer must be an instance of %s', \NeuralNet\Layer\Base::class), 10);
            }
        }
        $this->layers = $layers;
    }

    /**
     * {@inheritDoc}
     */
    public function result(array $input): array
    {
        // Распространение данных от входов к выходам:
        // Step 3. Каждый входной нейрон (x[i], i=1,2,..,n) отправляет полученный
        // сигнал x[i] всем нейронам в следующем слое (скрытом).
        foreach ($this->layers as $layer) {
            /* @var $layer \NeuralNet\Layer\Base */
            $output = $layer->result($input);
            // После чего посылает результат всем элементам следующего слоя (до выходного).
            $input = $output;
            // Step 5. Каждый выходной нейрон (Y[k], k=1,2,..,m) суммирует взвешенные
            // входящие сигналы: y_in[k] = w[0k] + SUM[j..m](z[j] * w[jk]) и
            // применяет активационную функцию, вычисляя выходной сигнал: y[k] = f(y_in[k])
        }
        return $input;
   }

}
